{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8228571",
   "metadata": {
    "papermill": {
     "duration": 0.002211,
     "end_time": "2026-01-25T15:07:24.698326",
     "exception": false,
     "start_time": "2026-01-25T15:07:24.696115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ”§ AI Telco Troubleshooting Challenge - Track 3\n",
    "\n",
    "- **Question**: Can our fine-tuned LLM detect and explain unseen network failures? \n",
    "\n",
    "    In other words, for the Track 3: Can we build a specialised edge-cloud LLM to troubleshoot network faults?\n",
    "\n",
    "- **Target**: Enhance the accuracy of Qwen2.5-1.5B-Instruct when answering telco troubleshooting questions in telelogs data.\n",
    "\n",
    "- **Objective**: Establish a baseline accuracy for Qwen2.5-1.5B-Instruct on telco troubleshooting questions using telelogs data, and quantify performance gains from domain-specific fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207fc8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T15:07:24.702798Z",
     "iopub.status.busy": "2026-01-25T15:07:24.702138Z",
     "iopub.status.idle": "2026-01-25T15:10:35.409333Z",
     "shell.execute_reply": "2026-01-25T15:10:35.408278Z"
    },
    "papermill": {
     "duration": 190.711678,
     "end_time": "2026-01-25T15:10:35.411511",
     "exception": false,
     "start_time": "2026-01-25T15:07:24.699833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.6/386.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "litellm 1.80.16 requires openai>=2.8.0, but you have openai 1.90.0 which is incompatible.\r\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\r\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q vllm==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1175bf34",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-25T15:10:35.509145Z",
     "iopub.status.busy": "2026-01-25T15:10:35.508856Z",
     "iopub.status.idle": "2026-01-25T15:10:35.518700Z",
     "shell.execute_reply": "2026-01-25T15:10:35.518113Z"
    },
    "papermill": {
     "duration": 0.051994,
     "end_time": "2026-01-25T15:10:35.520098",
     "exception": false,
     "start_time": "2026-01-25T15:10:35.468104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline.py\n",
    "\"\"\"\n",
    "TeleLogs Track 3 - Baseline Submission\n",
    "Author: Kodjo JosuÃ© AYITEY (Yehoshua)\n",
    "Generates submission.csv using Qwen2.5-1.5B-Instruct via vLLM\n",
    "Supports combined Phase 1 + Phase 2 test set\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    working_dir: Path = Path(\"/kaggle/working\")\n",
    "    input_dir: Path = Path(\"/kaggle/input/the-ai-telco-troubleshooting-challenge-dataset\")\n",
    "\n",
    "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    tensor_parallel_size: int = 2\n",
    "    gpu_memory_utilization: float = 0.90\n",
    "    max_model_len: int = 8192\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    max_tokens: int = 256\n",
    "    num_generations: int = 4   # Must match sample submission\n",
    "\n",
    "    output_path: Path = working_dir / \"submission.csv\"\n",
    "    sample_submission_path: Path = input_dir / \"SampleSubmission.csv\"\n",
    "\n",
    "    phase1_test_path: Path = input_dir / \"phase_1_test.csv\"\n",
    "    phase2_test_path: Path = input_dir / \"phase_2_test.csv\"\n",
    "\n",
    "    target_column: str = \"Qwen2.5-1.5B-Instruct\"\n",
    "    placeholder: str = \"placeholder\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ROBUST ANSWER EXTRACTION\n",
    "# =============================================================================\n",
    "def extract_answer(generated: str, fallback: str = \"C8\", verbose: bool = False) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Enhanced extraction to handle varied formats: C1-C8, M1-M5, P1-P8, 1-8, A-I, etc.\n",
    "\n",
    "    Returns: Tuple[str, str]: (predicted_answer, extraction_method_used)\n",
    "    \"\"\"\n",
    "    text = generated.strip()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n[DEBUG] Full generated text length: {len(text)} chars\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Priority 1: \\boxed{...} - Most strict, most reliable\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    boxed_pattern = r'\\\\boxed\\s*(?:\\{|\\s*)([^}]+?)(?:\\s*\\}|$)'\n",
    "    boxed_matches = list(re.finditer(boxed_pattern, text, re.IGNORECASE))\n",
    "\n",
    "    if boxed_matches:\n",
    "        for match in reversed(boxed_matches):\n",
    "            content = match.group(1).strip()\n",
    "            if verbose:\n",
    "                print(f\"[DEBUG] Found \\\\boxed{{{content}}}\")\n",
    "\n",
    "            # Flexible match: [A-Z]?[1-9] or [A-I]\n",
    "            code_match = re.search(r'\\b([A-Z]?\\d|[A-I])\\b', content, re.IGNORECASE)\n",
    "            if code_match:\n",
    "                return code_match.group(1).upper(), \"boxed\"\n",
    "\n",
    "            # Prefix like \"Option B\" or \"2:\"\n",
    "            code_prefix = re.match(r'^\\s*([A-Z]?\\d|[A-I])\\b', content, re.IGNORECASE)\n",
    "            if code_prefix:\n",
    "                return code_prefix.group(1).upper(), \"boxed_prefix\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[DEBUG] Found \\\\boxed{{}} but no valid code inside\")\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Priority 2: <answer>...</answer> tags\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    answer_pattern = r'<answer>\\s*([^<]+?)\\s*</answer>'\n",
    "    answer_blocks = re.findall(answer_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if answer_blocks:\n",
    "        for block in reversed(answer_blocks):\n",
    "            if verbose:\n",
    "                print(f\"[DEBUG] Found <answer>{block.strip()}</answer>\")\n",
    "\n",
    "            code_match = re.search(r'\\b([A-Z]?\\d|[A-I])\\b', block, re.IGNORECASE)\n",
    "            if code_match:\n",
    "                return code_match.group(1).upper(), \"answer_tag\"\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Priority 3: Final answer keywords followed by code\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    final_keywords = [\n",
    "        r'final\\s+answer\\s*:?\\s*([A-Z]?\\d|[A-I])\\b',\n",
    "        r'answer\\s*:?\\s*([A-Z]?\\d|[A-I])\\b',\n",
    "        r'therefore\\s*,?\\s*(?:the\\s+)?(?:answer\\s+is\\s*)?([A-Z]?\\d|[A-I])\\b',\n",
    "        r'conclusion\\s*:?\\s*([A-Z]?\\d|[A-I])\\b',\n",
    "        r'select\\s*:?\\s*([A-Z]?\\d|[A-I])\\b',\n",
    "        r'choose\\s*:?\\s*([A-Z]?\\d|[A-I])\\b',\n",
    "    ]\n",
    "\n",
    "    for pattern in final_keywords:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            last = matches[-1].group(1).upper()\n",
    "            if verbose:\n",
    "                print(f\"[DEBUG] Keyword match: {last}\")\n",
    "            return last, \"keyword\"\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Priority 4: Last sentence containing \"<code> is\" or \"<code> because\"\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    sentences = re.split(r'[.!?]\\s+', text)\n",
    "    for sentence in reversed(sentences):\n",
    "        if re.search(r'\\b([A-Z]?\\d|[A-I])\\s+(?:is|because|explains|accounts)\\b', sentence, re.IGNORECASE):\n",
    "            code_match = re.search(r'\\b([A-Z]?\\d|[A-I])\\b', sentence, re.IGNORECASE)\n",
    "            if code_match:\n",
    "                return code_match.group(1).upper(), \"contextual_sentence\"\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Priority 5: Last standalone code in text (weakest fallback)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    code_matches = re.findall(r'\\b([A-Z]?\\d|[A-I])\\b', text, re.IGNORECASE)\n",
    "\n",
    "    if code_matches:\n",
    "        last_code = code_matches[-1].upper()\n",
    "        unique_codes = set(c.upper() for c in code_matches)\n",
    "\n",
    "        if len(unique_codes) > 1 and verbose:\n",
    "            print(f\"[DEBUG] Multiple codes: {unique_codes}, using last: {last_code}\")\n",
    "\n",
    "        return last_code, \"last_occurrence\"\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # Ultimate fallback\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    if verbose:\n",
    "        print(\"[WARNING] No valid code found â†’ using fallback\")\n",
    "\n",
    "    return fallback, \"fallback\"\n",
    "\n",
    "\n",
    "def format_answer(cause: str) -> str:\n",
    "    return f\"Based on the provided data, the most likely root cause for the issue is: \\\\boxed{{{cause}}}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "def main():\n",
    "    config = Config()\n",
    "    config.working_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"ğŸš€ Starting baseline submission generation...\")\n",
    "\n",
    "    # Load sample submission template\n",
    "    if not config.sample_submission_path.exists():\n",
    "        raise FileNotFoundError(f\"SampleSubmission.csv not found at {config.sample_submission_path}\")\n",
    "\n",
    "    submission_df = pd.read_csv(config.sample_submission_path)\n",
    "    print(f\"Loaded submission template with {len(submission_df)} rows\")\n",
    "\n",
    "    # Load test questions from both phases\n",
    "    test_files = []\n",
    "    if config.phase1_test_path.exists():\n",
    "        test_files.append(config.phase1_test_path)\n",
    "    if config.phase2_test_path.exists():\n",
    "        test_files.append(config.phase2_test_path)\n",
    "\n",
    "    if not test_files:\n",
    "        raise FileNotFoundError(\"Neither phase_1_test.csv nor phase_2_test.csv found.\")\n",
    "\n",
    "    dfs = [pd.read_csv(p) for p in test_files]\n",
    "    test_df = pd.concat(dfs, ignore_index=True)\n",
    "    test_df = test_df.drop_duplicates(subset=[\"ID\"])  # safety\n",
    "    print(f\"Loaded {len(test_df)} base questions from test set(s)\")\n",
    "\n",
    "    # Prepare prompts\n",
    "    prompts = [f\"<|im_start|>user\\n{row['question']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "               for _, row in test_df.iterrows()]\n",
    "    base_ids = test_df[\"ID\"].tolist()\n",
    "\n",
    "    # Initialize model\n",
    "    print(f\"Loading model {config.model_name}...\")\n",
    "    llm = LLM(\n",
    "        model=config.model_name,\n",
    "        tensor_parallel_size=config.tensor_parallel_size,\n",
    "        gpu_memory_utilization=config.gpu_memory_utilization,\n",
    "        max_model_len=config.max_model_len,\n",
    "        trust_remote_code=True,\n",
    "        enforce_eager=True,\n",
    "        dtype=\"float16\",\n",
    "        disable_custom_all_reduce=True,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "        max_tokens=config.max_tokens,\n",
    "        n=config.num_generations,\n",
    "    )\n",
    "\n",
    "    # Generate responses\n",
    "    print(\"Generating responses...\")\n",
    "    outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "    # Fill submission\n",
    "    filled_count = 0\n",
    "    for base_id, output in zip(base_ids, outputs):\n",
    "        for i, completion in enumerate(output.outputs, 1):\n",
    "            row_id = f\"{base_id}_{i}\"\n",
    "            cause, method = extract_answer(completion.text, fallback=\"placeholder\")\n",
    "            # Optional: print(f\"{row_id} â†’ {cause} ({method})\")\n",
    "\n",
    "            answer_text = format_answer(cause)\n",
    "\n",
    "            mask = submission_df[\"ID\"] == row_id\n",
    "            if mask.any():\n",
    "                submission_df.loc[mask, config.target_column] = answer_text\n",
    "                filled_count += 1\n",
    "\n",
    "    # Fill placeholder columns\n",
    "    for col in submission_df.columns:\n",
    "        if col not in [\"ID\", config.target_column]:\n",
    "            submission_df[col] = config.placeholder\n",
    "\n",
    "    # Save\n",
    "    submission_df.to_csv(config.output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"âœ… Submission saved to {config.output_path}\")\n",
    "    print(f\"   Filled {filled_count} rows for {config.target_column}\")\n",
    "    print(f\"   Total rows in submission: {len(submission_df)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2d1dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T15:10:35.597434Z",
     "iopub.status.busy": "2026-01-25T15:10:35.596844Z",
     "iopub.status.idle": "2026-01-25T15:45:19.228705Z",
     "shell.execute_reply": "2026-01-25T15:45:19.227933Z"
    },
    "papermill": {
     "duration": 2083.67224,
     "end_time": "2026-01-25T15:45:19.230438",
     "exception": false,
     "start_time": "2026-01-25T15:10:35.558198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 15:10:47.369103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1769353847.573907      82 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1769353847.635565      82 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0000 00:00:1769353848.115044      82 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353848.115085      82 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353848.115092      82 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353848.115097      82 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "INFO 01-25 15:11:03 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "ğŸš€ Starting baseline submission generation...\r\n",
      "Loaded submission template with 6908 rows\r\n",
      "Loaded 1727 base questions from test set(s)\r\n",
      "Loading model Qwen/Qwen2.5-1.5B-Instruct...\r\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 660/660 [00:00<00:00, 5.14MB/s]\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "WARNING 01-25 15:11:19 [config.py:3443] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 01-25 15:11:19 [config.py:1604] Using max model len 8192\r\n",
      "WARNING 01-25 15:11:19 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "WARNING 01-25 15:11:21 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 01-25 15:11:21 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "tokenizer_config.json: 7.30kB [00:00, 31.8MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 50.2MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 131MB/s]\r\n",
      "tokenizer.json: 7.03MB [00:00, 189MB/s]\r\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:00<00:00, 1.64MB/s]\r\n",
      "WARNING 01-25 15:11:22 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\r\n",
      "WARNING 01-25 15:11:22 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 01-25 15:11:24 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 01-25 15:11:24 [cuda.py:395] Using XFormers backend.\r\n",
      "2026-01-25 15:11:27.463886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1769353887.484704     121 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1769353887.490965     121 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0000 00:00:1769353887.507158     121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353887.507198     121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353887.507204     121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1769353887.507208     121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "INFO 01-25 15:11:33 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:34 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:35 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:35 [cuda.py:395] Using XFormers backend.\r\n",
      "[W125 15:11:36.146670388 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W125 15:11:36.147461845 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W125 15:11:36.505795736 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W125 15:11:36.506484375 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:36 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:36 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 01-25 15:11:36 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "INFO 01-25 15:11:36 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 01-25 15:11:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_d2d33957'), local_subscribe_addr='ipc:///tmp/2c3e78b1-2ce6-410d-b0b1-c182b7f3c5e4', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:37 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 01-25 15:11:37 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 01-25 15:11:37 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:37 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\r\n",
      "INFO 01-25 15:11:37 [weight_utils.py:296] Using model weights format ['*.safetensors']\r\n",
      "model.safetensors:   0%|                            | 0.00/3.09G [00:00<?, ?B/s]\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:38 [weight_utils.py:296] Using model weights format ['*.safetensors']\r\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.09G/3.09G [00:14<00:00, 210MB/s]\r\n",
      "INFO 01-25 15:11:52 [weight_utils.py:312] Time spent downloading weights for Qwen/Qwen2.5-1.5B-Instruct: 14.883160 seconds\r\n",
      "INFO 01-25 15:11:52 [weight_utils.py:349] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.47s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.47s/it]\r\n",
      "\r\n",
      "INFO 01-25 15:11:54 [default_loader.py:262] Loading weights took 1.53 seconds\r\n",
      "INFO 01-25 15:11:55 [model_runner.py:1115] Model loading took 1.4479 GiB and 16.703799 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:59 [weight_utils.py:312] Time spent downloading weights for Qwen/Qwen2.5-1.5B-Instruct: 7.244158 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:11:59 [weight_utils.py:349] No model.safetensors.index.json found in remote.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:12:01 [default_loader.py:262] Loading weights took 1.62 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:12:02 [model_runner.py:1115] Model loading took 1.4479 GiB and 23.743659 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:12:08 [worker.py:295] model weights take 1.45GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.33GiB; the rest of the memory reserved for KV Cache is 11.38GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 01-25 15:12:08 [worker.py:295] model weights take 1.45GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 10.30GiB.\r\n",
      "INFO 01-25 15:12:08 [executor_base.py:113] # cuda blocks: 48229, # CPU blocks: 18724\r\n",
      "INFO 01-25 15:12:08 [executor_base.py:118] Maximum concurrency for 8192 tokens per request: 94.20x\r\n",
      "INFO 01-25 15:12:14 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 12.28 seconds\r\n",
      "Generating responses...\r\n",
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1727/1727 [00:09<00:00, 183.62it/s]\r\n",
      "Processed prompts: 100%|â–ˆ| 6908/6908 [32:40<00:00,  3.52it/s, est. speed input: \r\n",
      "âœ… Submission saved to /kaggle/working/submission.csv\r\n",
      "   Filled 6908 rows for Qwen2.5-1.5B-Instruct\r\n",
      "   Total rows in submission: 6908\r\n",
      "INFO 01-25 15:45:11 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 01-25 15:45:11 [multiproc_worker_utils.py:260] Worker exiting\r\n",
      "/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\r\n",
      "[rank0]:[W125 15:45:14.062808630 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "!python baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5f894f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T15:45:19.364851Z",
     "iopub.status.busy": "2026-01-25T15:45:19.364551Z",
     "iopub.status.idle": "2026-01-25T15:45:19.367945Z",
     "shell.execute_reply": "2026-01-25T15:45:19.367368Z"
    },
    "papermill": {
     "duration": 0.071303,
     "end_time": "2026-01-25T15:45:19.369239",
     "exception": false,
     "start_time": "2026-01-25T15:45:19.297936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9189369,
     "sourceId": 14607249,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2277.673126,
   "end_time": "2026-01-25T15:45:19.652898",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-25T15:07:21.979772",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
