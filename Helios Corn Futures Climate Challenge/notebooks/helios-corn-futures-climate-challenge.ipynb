{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89627c67",
   "metadata": {
    "papermill": {
     "duration": 0.008992,
     "end_time": "2026-01-30T17:49:34.918209",
     "exception": false,
     "start_time": "2026-01-30T17:49:34.909217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üåΩ Helios Corn Futures Climate Challenge: Advanced Feature Engineering & Signal Selection\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Overview & Objectives\n",
    "This solution extends the foundational methodology established in the [Starter Notebook](https://www.kaggle.com/code/erguntiryaki/starter-notebook-with-baseline) by implementing a high-fidelity feature engineering pipeline specifically optimized for the **Climate-Futures Correlation Score (CFCS)**.\n",
    "\n",
    "Our primary objective was to move beyond raw risk counts toward **economically meaningful signals** that capture the temporal, seasonal, and non-linear relationship between climate stress and commodity markets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Technical Implementation: Feature Engineering\n",
    "\n",
    "| Category | Implementation | Logical Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **Temporal Dynamics** | 7, 14, 30, 60, 90-day Lags | Weather events exhibit a delayed impact on futures pricing and market sentiment. |\n",
    "| **Smoothing & Trend** | Exponential Moving Averages (EMA) | Reduces daily noise while preserving the momentum of developing climate risks. |\n",
    "| **Market Volatility** | Rolling Standard Deviation (7-46 days) | Captures climate \"instability\" as a proxy for market uncertainty and price variance. |\n",
    "| **Cumulative Impact** | Rolling Summation (30-90 days) | Total accumulated stress (e.g., prolonged drought) often has a threshold effect on crop yields. |\n",
    "| **Non-linear Effects** | Squared & Cross-risk Interactions | Models extreme weather events and synergistic stressors (e.g., Simultaneous Heat & Drought). |\n",
    "| **Seasonality** | Sin/Cos Cyclical Encoding | Embeds the biological constraints of the corn growing season into the feature space. |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Strategy: Signal-to-Noise Optimization (CFCS-Centric)\n",
    "\n",
    "The **CFCS** metric penalizes the inclusion of low-signal features through its denominator:\n",
    "\n",
    "$$CFCS = (0.5 \\times Avg\\_Sig\\_Corr) + (0.3 \\times Max\\_Corr) + (0.2 \\times \\frac{Sig\\_Count}{Total\\_Features \\times \\dots})$$\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> **Key Insight:** Redundant or weak features act as \"noise,\" inflating the denominator and diluting the `Sig_Count%`. Our strategy shifts from *Feature Generation* to *Feature Pruning*.\n",
    "\n",
    "#### The Iterative Pruning Pipeline:\n",
    "1.  **Generation:** Synthesize 100+ advanced features across multiple look-back windows.\n",
    "2.  **Analysis:** Measure the specific contribution of each feature to significant correlations ($\\ge 0.5$).\n",
    "3.  **Filtration:** Systematically remove all features with zero significant correlations.\n",
    "4.  **Refinement:** Retain only a sparse, high-conviction feature set that maximizes the `Avg_Sig_Corr` without compromising the `Sig_Count` density.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Future Work\n",
    "While this pipeline provides a robust baseline, there is significant potential in:\n",
    "- Exploring regional-specific production weights for more granular feature aggregation.\n",
    "- Investigating lead-lag relationships between agricultural commodities.\n",
    "\n",
    "---\n",
    "*Created for the Helios Competition Host Review*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e482c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.12.12\n",
    "# Kaggle requirements.txt exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df800a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:34.934548Z",
     "iopub.status.busy": "2026-01-30T17:49:34.934176Z",
     "iopub.status.idle": "2026-01-30T17:49:37.868746Z",
     "shell.execute_reply": "2026-01-30T17:49:37.867580Z"
    },
    "papermill": {
     "duration": 2.945384,
     "end_time": "2026-01-30T17:49:37.870931",
     "exception": false,
     "start_time": "2026-01-30T17:49:34.925547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc4a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:37.888581Z",
     "iopub.status.busy": "2026-01-30T17:49:37.888104Z",
     "iopub.status.idle": "2026-01-30T17:49:40.717207Z",
     "shell.execute_reply": "2026-01-30T17:49:40.715996Z"
    },
    "papermill": {
     "duration": 2.840402,
     "end_time": "2026-01-30T17:49:40.719347",
     "exception": false,
     "start_time": "2026-01-30T17:49:37.878945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset: 320,661 rows\n",
      "üìÖ Date range: 2016-01-01 00:00:00 to 2025-12-15 00:00:00\n",
      "üåç Countries: 11\n",
      "üìç Regions: 89\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RISK_CATEGORIES = ['heat_stress', 'unseasonably_cold', 'excess_precip', 'drought']\n",
    "SIGNIFICANCE_THRESHOLD = 0.5\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = '/kaggle/input/forecasting-the-future-the-helios-corn-climate-challenge/'\n",
    "OUTPUT_PATH = '/kaggle/working/'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(f'{DATA_PATH}corn_climate_risk_futures_daily_master.csv')\n",
    "df['date_on'] = pd.to_datetime(df['date_on'])\n",
    "market_share_df = pd.read_csv(f'{DATA_PATH}corn_regional_market_share.csv')\n",
    "\n",
    "print(f\"üìä Dataset: {len(df):,} rows\")\n",
    "print(f\"üìÖ Date range: {df['date_on'].min()} to {df['date_on'].max()}\")\n",
    "print(f\"üåç Countries: {df['country_name'].nunique()}\")\n",
    "print(f\"üìç Regions: {df['region_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e43ae",
   "metadata": {
    "papermill": {
     "duration": 0.007072,
     "end_time": "2026-01-30T17:49:40.735061",
     "exception": false,
     "start_time": "2026-01-30T17:49:40.727989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üìä Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dbfc2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:40.752492Z",
     "iopub.status.busy": "2026-01-30T17:49:40.752118Z",
     "iopub.status.idle": "2026-01-30T17:49:40.772062Z",
     "shell.execute_reply": "2026-01-30T17:49:40.770938Z"
    },
    "papermill": {
     "duration": 0.031042,
     "end_time": "2026-01-30T17:49:40.774299",
     "exception": false,
     "start_time": "2026-01-30T17:49:40.743257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_cfcs(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute CFCS score for a dataframe.\n",
    "    CFCS = (0.5 √ó Avg_Sig_Corr) + (0.3 √ó Max_Corr) + (0.2 √ó Sig_Count%)\n",
    "    \"\"\"\n",
    "    climate_cols = [c for c in df.columns if c.startswith(\"climate_risk_\")]\n",
    "    futures_cols = [c for c in df.columns if c.startswith(\"futures_\")]\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    for country in df['country_name'].unique():\n",
    "        df_country = df[df['country_name'] == country]\n",
    "        \n",
    "        for month in df_country['date_on_month'].unique():\n",
    "            df_month = df_country[df_country['date_on_month'] == month]\n",
    "            \n",
    "            for clim in climate_cols:\n",
    "                for fut in futures_cols:\n",
    "                    if df_month[clim].std() > 0 and df_month[fut].std() > 0:\n",
    "                        corr = df_month[[clim, fut]].corr().iloc[0, 1]\n",
    "                        correlations.append(corr)\n",
    "    \n",
    "    correlations = pd.Series(correlations).dropna()\n",
    "    abs_corrs = correlations.abs()\n",
    "    sig_corrs = abs_corrs[abs_corrs >= SIGNIFICANCE_THRESHOLD]\n",
    "    \n",
    "    avg_sig = sig_corrs.mean() if len(sig_corrs) > 0 else 0\n",
    "    max_corr = abs_corrs.max() if len(abs_corrs) > 0 else 0\n",
    "    sig_pct = len(sig_corrs) / len(correlations) * 100 if len(correlations) > 0 else 0\n",
    "    \n",
    "    avg_sig_score = min(100, avg_sig * 100)\n",
    "    max_score = min(100, max_corr * 100)\n",
    "    \n",
    "    cfcs = (0.5 * avg_sig_score) + (0.3 * max_score) + (0.2 * sig_pct)\n",
    "    \n",
    "    result = {\n",
    "        'cfcs': round(cfcs, 2),\n",
    "        'avg_sig_corr': round(avg_sig, 4),\n",
    "        'max_corr': round(max_corr, 4),\n",
    "        'sig_count': len(sig_corrs),\n",
    "        'total': len(correlations),\n",
    "        'sig_pct': round(sig_pct, 4),\n",
    "        'n_features': len(climate_cols)\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"CFCS: {result['cfcs']} | Sig: {result['sig_count']}/{result['total']} ({result['sig_pct']:.2f}%) | Features: {result['n_features']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def analyze_feature_contributions(df, climate_cols, futures_cols):\n",
    "    \"\"\"\n",
    "    Analyze contribution of each climate feature.\n",
    "    Returns DataFrame with sig_count, max_corr, etc for each feature.\n",
    "    \"\"\"\n",
    "    feature_stats = {col: {'sig_count': 0, 'total': 0, 'max_corr': 0, 'sig_corrs': []} \n",
    "                     for col in climate_cols}\n",
    "    \n",
    "    for country in df['country_name'].unique():\n",
    "        df_country = df[df['country_name'] == country]\n",
    "        \n",
    "        for month in df_country['date_on_month'].unique():\n",
    "            df_month = df_country[df_country['date_on_month'] == month]\n",
    "            \n",
    "            for clim in climate_cols:\n",
    "                for fut in futures_cols:\n",
    "                    if df_month[clim].std() > 0 and df_month[fut].std() > 0:\n",
    "                        corr = df_month[[clim, fut]].corr().iloc[0, 1]\n",
    "                        \n",
    "                        feature_stats[clim]['total'] += 1\n",
    "                        \n",
    "                        if abs(corr) >= SIGNIFICANCE_THRESHOLD:\n",
    "                            feature_stats[clim]['sig_count'] += 1\n",
    "                            feature_stats[clim]['sig_corrs'].append(abs(corr))\n",
    "                        \n",
    "                        if abs(corr) > feature_stats[clim]['max_corr']:\n",
    "                            feature_stats[clim]['max_corr'] = abs(corr)\n",
    "    \n",
    "    results = []\n",
    "    for col, stats in feature_stats.items():\n",
    "        avg_sig = np.mean(stats['sig_corrs']) if stats['sig_corrs'] else 0\n",
    "        results.append({\n",
    "            'feature': col,\n",
    "            'sig_count': stats['sig_count'],\n",
    "            'total': stats['total'],\n",
    "            'sig_pct': stats['sig_count'] / stats['total'] * 100 if stats['total'] > 0 else 0,\n",
    "            'max_corr': round(stats['max_corr'], 4),\n",
    "            'avg_sig_corr': round(avg_sig, 4)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('sig_count', ascending=False)\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a0394",
   "metadata": {
    "papermill": {
     "duration": 0.007181,
     "end_time": "2026-01-30T17:49:40.788928",
     "exception": false,
     "start_time": "2026-01-30T17:49:40.781747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 1: Base Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f87716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:40.804941Z",
     "iopub.status.busy": "2026-01-30T17:49:40.804566Z",
     "iopub.status.idle": "2026-01-30T17:49:41.054104Z",
     "shell.execute_reply": "2026-01-30T17:49:41.052965Z"
    },
    "papermill": {
     "duration": 0.260174,
     "end_time": "2026-01-30T17:49:41.056308",
     "exception": false,
     "start_time": "2026-01-30T17:49:40.796134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base setup complete\n"
     ]
    }
   ],
   "source": [
    "# Create working copy\n",
    "merged_df = df.copy()\n",
    "\n",
    "# Add time features\n",
    "merged_df['day_of_year'] = merged_df['date_on'].dt.dayofyear\n",
    "merged_df['quarter'] = merged_df['date_on'].dt.quarter\n",
    "\n",
    "# Merge market share\n",
    "merged_df = merged_df.merge(\n",
    "    market_share_df[['region_id', 'percent_country_production']], \n",
    "    on='region_id', how='left'\n",
    ")\n",
    "merged_df['percent_country_production'] = merged_df['percent_country_production'].fillna(1.0)\n",
    "\n",
    "# Track all created features\n",
    "ALL_NEW_FEATURES = []\n",
    "\n",
    "print(\"‚úÖ Base setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5511e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:41.074471Z",
     "iopub.status.busy": "2026-01-30T17:49:41.074126Z",
     "iopub.status.idle": "2026-01-30T17:49:41.113963Z",
     "shell.execute_reply": "2026-01-30T17:49:41.112873Z"
    },
    "papermill": {
     "duration": 0.051637,
     "end_time": "2026-01-30T17:49:41.116220",
     "exception": false,
     "start_time": "2026-01-30T17:49:41.064583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base risk scores: 8 features\n"
     ]
    }
   ],
   "source": [
    "# Base Risk Scores\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    low_col = f'climate_risk_cnt_locations_{risk_type}_risk_low'\n",
    "    med_col = f'climate_risk_cnt_locations_{risk_type}_risk_medium' \n",
    "    high_col = f'climate_risk_cnt_locations_{risk_type}_risk_high'\n",
    "    \n",
    "    total = merged_df[low_col] + merged_df[med_col] + merged_df[high_col]\n",
    "    risk_score = (merged_df[med_col] + 2 * merged_df[high_col]) / (total + 1e-6)\n",
    "    weighted = risk_score * (merged_df['percent_country_production'] / 100)\n",
    "    \n",
    "    merged_df[f'climate_risk_{risk_type}_score'] = risk_score\n",
    "    merged_df[f'climate_risk_{risk_type}_weighted'] = weighted\n",
    "    ALL_NEW_FEATURES.extend([f'climate_risk_{risk_type}_score', f'climate_risk_{risk_type}_weighted'])\n",
    "\n",
    "print(f\"‚úÖ Base risk scores: {len(ALL_NEW_FEATURES)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec2337",
   "metadata": {
    "papermill": {
     "duration": 0.007374,
     "end_time": "2026-01-30T17:49:41.131061",
     "exception": false,
     "start_time": "2026-01-30T17:49:41.123687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 2: Advanced Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dffd8d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:41.147405Z",
     "iopub.status.busy": "2026-01-30T17:49:41.147043Z",
     "iopub.status.idle": "2026-01-30T17:49:44.254393Z",
     "shell.execute_reply": "2026-01-30T17:49:44.253165Z"
    },
    "papermill": {
     "duration": 3.11917,
     "end_time": "2026-01-30T17:49:44.257383",
     "exception": false,
     "start_time": "2026-01-30T17:49:41.138213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rolling features: 48 total\n"
     ]
    }
   ],
   "source": [
    "# Sort for time series operations\n",
    "merged_df = merged_df.sort_values(['region_id', 'date_on'])\n",
    "\n",
    "# Rolling MA and Max (7, 14, 30, 60 days, 90 days)\n",
    "for window in [7, 14, 30, 60, 90]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        \n",
    "        # Moving Average\n",
    "        ma_col = f'climate_risk_{risk_type}_ma_{window}d'\n",
    "        merged_df[ma_col] = (\n",
    "            merged_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        )\n",
    "        ALL_NEW_FEATURES.append(ma_col)\n",
    "        \n",
    "        # Rolling Max\n",
    "        max_col = f'climate_risk_{risk_type}_max_{window}d'\n",
    "        merged_df[max_col] = (\n",
    "            merged_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        )\n",
    "        ALL_NEW_FEATURES.append(max_col)\n",
    "\n",
    "print(f\"‚úÖ Rolling features: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8b081",
   "metadata": {
    "papermill": {
     "duration": 0.00826,
     "end_time": "2026-01-30T17:49:44.273787",
     "exception": false,
     "start_time": "2026-01-30T17:49:44.265527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 3: Lag Features (Weather Affects Prices with Delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0896a45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:44.291588Z",
     "iopub.status.busy": "2026-01-30T17:49:44.291242Z",
     "iopub.status.idle": "2026-01-30T17:49:45.112305Z",
     "shell.execute_reply": "2026-01-30T17:49:45.111120Z"
    },
    "papermill": {
     "duration": 0.832425,
     "end_time": "2026-01-30T17:49:45.114475",
     "exception": false,
     "start_time": "2026-01-30T17:49:44.282050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lag features added: 68 total\n"
     ]
    }
   ],
   "source": [
    "# Lag features - weather today affects prices in future\n",
    "for lag in [7, 14, 30, 60, 90]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        \n",
    "        lag_col = f'climate_risk_{risk_type}_lag_{lag}d'\n",
    "        merged_df[lag_col] = merged_df.groupby('region_id')[score_col].shift(lag)\n",
    "        ALL_NEW_FEATURES.append(lag_col)\n",
    "\n",
    "print(f\"‚úÖ Lag features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db438ddd",
   "metadata": {
    "papermill": {
     "duration": 0.007301,
     "end_time": "2026-01-30T17:49:45.129266",
     "exception": false,
     "start_time": "2026-01-30T17:49:45.121965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 4: EMA Features (More Weight to Recent Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59291dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:45.147331Z",
     "iopub.status.busy": "2026-01-30T17:49:45.145965Z",
     "iopub.status.idle": "2026-01-30T17:49:45.977568Z",
     "shell.execute_reply": "2026-01-30T17:49:45.976447Z"
    },
    "papermill": {
     "duration": 0.843207,
     "end_time": "2026-01-30T17:49:45.979859",
     "exception": false,
     "start_time": "2026-01-30T17:49:45.136652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EMA features added: 80 total\n"
     ]
    }
   ],
   "source": [
    "# Exponential Moving Averages\n",
    "for span in [14, 30, 46]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        \n",
    "        ema_col = f'climate_risk_{risk_type}_ema_{span}d'\n",
    "        merged_df[ema_col] = (\n",
    "            merged_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.ewm(span=span, min_periods=1).mean())\n",
    "        )\n",
    "        ALL_NEW_FEATURES.append(ema_col)\n",
    "\n",
    "print(f\"‚úÖ EMA features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28b688",
   "metadata": {
    "papermill": {
     "duration": 0.009819,
     "end_time": "2026-01-30T17:49:45.997108",
     "exception": false,
     "start_time": "2026-01-30T17:49:45.987289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 5: Volatility Features (Risk Variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a019c900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:46.014919Z",
     "iopub.status.busy": "2026-01-30T17:49:46.014555Z",
     "iopub.status.idle": "2026-01-30T17:49:46.955180Z",
     "shell.execute_reply": "2026-01-30T17:49:46.954041Z"
    },
    "papermill": {
     "duration": 0.951789,
     "end_time": "2026-01-30T17:49:46.957182",
     "exception": false,
     "start_time": "2026-01-30T17:49:46.005393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Volatility features added: 92 total\n"
     ]
    }
   ],
   "source": [
    "# Rolling Standard Deviation (volatility)\n",
    "for window in [14, 30, 46]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        \n",
    "        vol_col = f'climate_risk_{risk_type}_vol_{window}d'\n",
    "        merged_df[vol_col] = (\n",
    "            merged_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=2).std())\n",
    "        )\n",
    "        ALL_NEW_FEATURES.append(vol_col)\n",
    "\n",
    "print(f\"‚úÖ Volatility features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31bb3f",
   "metadata": {
    "papermill": {
     "duration": 0.007378,
     "end_time": "2026-01-30T17:49:46.972303",
     "exception": false,
     "start_time": "2026-01-30T17:49:46.964925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 6: Cumulative Stress Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc4cc68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:46.990869Z",
     "iopub.status.busy": "2026-01-30T17:49:46.989615Z",
     "iopub.status.idle": "2026-01-30T17:49:47.857399Z",
     "shell.execute_reply": "2026-01-30T17:49:47.856317Z"
    },
    "papermill": {
     "duration": 0.879845,
     "end_time": "2026-01-30T17:49:47.859473",
     "exception": false,
     "start_time": "2026-01-30T17:49:46.979628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cumulative features added: 104 total\n"
     ]
    }
   ],
   "source": [
    "# Cumulative sum (total stress over period)\n",
    "for window in [30, 60, 90]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        \n",
    "        cum_col = f'climate_risk_{risk_type}_cumsum_{window}d'\n",
    "        merged_df[cum_col] = (\n",
    "            merged_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "        )\n",
    "        ALL_NEW_FEATURES.append(cum_col)\n",
    "\n",
    "print(f\"‚úÖ Cumulative features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9b2d8",
   "metadata": {
    "papermill": {
     "duration": 0.009722,
     "end_time": "2026-01-30T17:49:47.876773",
     "exception": false,
     "start_time": "2026-01-30T17:49:47.867051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 7: Non-linear Features (Extreme Events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73268341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:47.894839Z",
     "iopub.status.busy": "2026-01-30T17:49:47.894401Z",
     "iopub.status.idle": "2026-01-30T17:49:47.933270Z",
     "shell.execute_reply": "2026-01-30T17:49:47.932278Z"
    },
    "papermill": {
     "duration": 0.050627,
     "end_time": "2026-01-30T17:49:47.935432",
     "exception": false,
     "start_time": "2026-01-30T17:49:47.884805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Non-linear features added: 112 total\n"
     ]
    }
   ],
   "source": [
    "# Non-linear transformations\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    \n",
    "    # Squared - emphasizes extreme values\n",
    "    sq_col = f'climate_risk_{risk_type}_squared'\n",
    "    merged_df[sq_col] = merged_df[score_col] ** 2\n",
    "    ALL_NEW_FEATURES.append(sq_col)\n",
    "    \n",
    "    # Log transform - compresses high values\n",
    "    log_col = f'climate_risk_{risk_type}_log'\n",
    "    merged_df[log_col] = np.log1p(merged_df[score_col])\n",
    "    ALL_NEW_FEATURES.append(log_col)\n",
    "\n",
    "print(f\"‚úÖ Non-linear features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae56d2",
   "metadata": {
    "papermill": {
     "duration": 0.007472,
     "end_time": "2026-01-30T17:49:47.950548",
     "exception": false,
     "start_time": "2026-01-30T17:49:47.943076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 8: Interaction Features (Combined Stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32e5f297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:47.967622Z",
     "iopub.status.busy": "2026-01-30T17:49:47.967269Z",
     "iopub.status.idle": "2026-01-30T17:49:48.186792Z",
     "shell.execute_reply": "2026-01-30T17:49:48.185607Z"
    },
    "papermill": {
     "duration": 0.23086,
     "end_time": "2026-01-30T17:49:48.188980",
     "exception": false,
     "start_time": "2026-01-30T17:49:47.958120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interaction features added: 119 total\n"
     ]
    }
   ],
   "source": [
    "# Composite indices\n",
    "score_cols = [f'climate_risk_{r}_score' for r in RISK_CATEGORIES]\n",
    "\n",
    "# Temperature stress (max of heat/cold)\n",
    "merged_df['climate_risk_temperature_stress'] = merged_df[[\n",
    "    'climate_risk_heat_stress_score', 'climate_risk_unseasonably_cold_score'\n",
    "]].max(axis=1)\n",
    "ALL_NEW_FEATURES.append('climate_risk_temperature_stress')\n",
    "\n",
    "# Precipitation stress (max of wet/dry)\n",
    "merged_df['climate_risk_precipitation_stress'] = merged_df[[\n",
    "    'climate_risk_excess_precip_score', 'climate_risk_drought_score'\n",
    "]].max(axis=1)\n",
    "ALL_NEW_FEATURES.append('climate_risk_precipitation_stress')\n",
    "\n",
    "# Overall stress (max of all)\n",
    "merged_df['climate_risk_overall_stress'] = merged_df[score_cols].max(axis=1)\n",
    "ALL_NEW_FEATURES.append('climate_risk_overall_stress')\n",
    "\n",
    "# Combined stress (sum of all)\n",
    "merged_df['climate_risk_combined_stress'] = merged_df[score_cols].sum(axis=1)\n",
    "ALL_NEW_FEATURES.append('climate_risk_combined_stress')\n",
    "\n",
    "# Difference features\n",
    "merged_df['climate_risk_precip_drought_diff'] = (\n",
    "    merged_df['climate_risk_excess_precip_score'] - merged_df['climate_risk_drought_score']\n",
    ")\n",
    "ALL_NEW_FEATURES.append('climate_risk_precip_drought_diff')\n",
    "\n",
    "merged_df['climate_risk_temp_diff'] = (\n",
    "    merged_df['climate_risk_heat_stress_score'] - merged_df['climate_risk_unseasonably_cold_score']\n",
    ")\n",
    "ALL_NEW_FEATURES.append('climate_risk_temp_diff')\n",
    "\n",
    "# Ratio features\n",
    "merged_df['climate_risk_precip_drought_ratio'] = (\n",
    "    merged_df['climate_risk_excess_precip_score'] / \n",
    "    (merged_df['climate_risk_drought_score'] + 0.01)\n",
    ")\n",
    "ALL_NEW_FEATURES.append('climate_risk_precip_drought_ratio')\n",
    "\n",
    "print(f\"‚úÖ Interaction features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442e0f6",
   "metadata": {
    "papermill": {
     "duration": 0.008121,
     "end_time": "2026-01-30T17:49:48.205098",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.196977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 9: Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0896ae18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:48.223674Z",
     "iopub.status.busy": "2026-01-30T17:49:48.222668Z",
     "iopub.status.idle": "2026-01-30T17:49:48.253118Z",
     "shell.execute_reply": "2026-01-30T17:49:48.252050Z"
    },
    "papermill": {
     "duration": 0.041971,
     "end_time": "2026-01-30T17:49:48.255273",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.213302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Seasonal features added: 123 total\n"
     ]
    }
   ],
   "source": [
    "# Cyclical encoding of day of year\n",
    "merged_df['climate_risk_season_sin'] = np.sin(2 * np.pi * merged_df['day_of_year'] / 365)\n",
    "merged_df['climate_risk_season_cos'] = np.cos(2 * np.pi * merged_df['day_of_year'] / 365)\n",
    "ALL_NEW_FEATURES.extend(['climate_risk_season_sin', 'climate_risk_season_cos'])\n",
    "\n",
    "# Growing season weighted risk (Q2-Q3 higher weight)\n",
    "growing_season_weight = merged_df['quarter'].map({1: 0.5, 2: 1.0, 3: 1.0, 4: 0.5})\n",
    "\n",
    "for risk_type in ['drought', 'excess_precip']:  # Most relevant for growing season\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    seasonal_col = f'climate_risk_{risk_type}_seasonal'\n",
    "    merged_df[seasonal_col] = merged_df[score_col] * growing_season_weight\n",
    "    ALL_NEW_FEATURES.append(seasonal_col)\n",
    "\n",
    "print(f\"‚úÖ Seasonal features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e358fb",
   "metadata": {
    "papermill": {
     "duration": 0.007622,
     "end_time": "2026-01-30T17:49:48.270782",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.263160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 10: Momentum Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23479881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:48.288548Z",
     "iopub.status.busy": "2026-01-30T17:49:48.288182Z",
     "iopub.status.idle": "2026-01-30T17:49:48.801337Z",
     "shell.execute_reply": "2026-01-30T17:49:48.800082Z"
    },
    "papermill": {
     "duration": 0.525236,
     "end_time": "2026-01-30T17:49:48.803548",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.278312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Momentum features added: 135 total\n"
     ]
    }
   ],
   "source": [
    "# Momentum/change features\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    \n",
    "    # Daily change\n",
    "    c1 = f'climate_risk_{risk_type}_change_1d'\n",
    "    merged_df[c1] = merged_df.groupby('region_id')[score_col].diff(1)\n",
    "    ALL_NEW_FEATURES.append(c1)\n",
    "    \n",
    "    # Weekly change\n",
    "    c7 = f'climate_risk_{risk_type}_change_7d'\n",
    "    merged_df[c7] = merged_df.groupby('region_id')[score_col].diff(7)\n",
    "    ALL_NEW_FEATURES.append(c7)\n",
    "    \n",
    "    # Acceleration\n",
    "    acc = f'climate_risk_{risk_type}_acceleration'\n",
    "    merged_df[acc] = merged_df.groupby('region_id')[c1].diff(1)\n",
    "    ALL_NEW_FEATURES.append(acc)\n",
    "\n",
    "print(f\"‚úÖ Momentum features added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aee423",
   "metadata": {
    "papermill": {
     "duration": 0.00838,
     "end_time": "2026-01-30T17:49:48.819700",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.811320",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üîß Phase 11: Country Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fb8e5af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:48.838166Z",
     "iopub.status.busy": "2026-01-30T17:49:48.837782Z",
     "iopub.status.idle": "2026-01-30T17:49:51.403369Z",
     "shell.execute_reply": "2026-01-30T17:49:51.401517Z"
    },
    "papermill": {
     "duration": 2.577468,
     "end_time": "2026-01-30T17:49:51.405453",
     "exception": false,
     "start_time": "2026-01-30T17:49:48.827985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Country aggregations added: 155 total\n"
     ]
    }
   ],
   "source": [
    "# Country-level aggregations\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    weighted_col = f'climate_risk_{risk_type}_weighted'\n",
    "    \n",
    "    country_agg = merged_df.groupby(['country_name', 'date_on']).agg({\n",
    "        score_col: ['mean', 'max', 'std'],\n",
    "        weighted_col: 'sum',\n",
    "        'percent_country_production': 'sum'\n",
    "    }).round(4)\n",
    "    \n",
    "    country_agg.columns = [f'country_{risk_type}_{\"_\".join(col).strip()}' for col in country_agg.columns]\n",
    "    country_agg = country_agg.reset_index()\n",
    "    \n",
    "    new_cols = [c for c in country_agg.columns if c not in ['country_name', 'date_on']]\n",
    "    ALL_NEW_FEATURES.extend(new_cols)\n",
    "    \n",
    "    merged_df = merged_df.merge(country_agg, on=['country_name', 'date_on'], how='left')\n",
    "\n",
    "print(f\"‚úÖ Country aggregations added: {len(ALL_NEW_FEATURES)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a458b4ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:51.425606Z",
     "iopub.status.busy": "2026-01-30T17:49:51.425051Z",
     "iopub.status.idle": "2026-01-30T17:49:59.431943Z",
     "shell.execute_reply": "2026-01-30T17:49:59.430824Z"
    },
    "papermill": {
     "duration": 8.020225,
     "end_time": "2026-01-30T17:49:59.434371",
     "exception": false,
     "start_time": "2026-01-30T17:49:51.414146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Before NaN handling: 320,661 rows\n",
      "üìä Identifying valid IDs (simulating sample submission)...\n",
      "üìä Valid IDs from sample submission approach: 219,161\n",
      "üìä Filling engineered features with 0...\n",
      "üìä Filtering to valid IDs...\n",
      "üìä After NaN handling: 219,161 rows\n",
      "üìä Expected rows: 219,161\n",
      "üìä Match: ‚úÖ\n",
      "üìä Total new features: 155\n"
     ]
    }
   ],
   "source": [
    "# Since feature engineering creates some new NaN values due to lag etc. it might be tricky to\n",
    "# match the IDs Kaggle expects.\n",
    "# Although being far from optimal below approach guarantees exactly 219,161 rows while preserving all feature values.\n",
    "#### STEPS FOLLOWED BELOW ####\n",
    "# 1. Simulate what sample submission does to identify valid rows (by ID)\n",
    "# 2. Fill all engineered features with 0 (edge-effect NaN)\n",
    "# 3. Filter to only keep rows with valid IDs\n",
    "\n",
    "REQUIRED_ROWS = 219161\n",
    "\n",
    "print(f\"\\nüìä Before NaN handling: {len(merged_df):,} rows\")\n",
    "\n",
    "# Step 1: Identify valid IDs by simulating sample submission's approach\n",
    "print(\"üìä Identifying valid IDs (simulating sample submission)...\")\n",
    "\n",
    "# Start fresh from original data\n",
    "temp_df = pd.read_csv(f'{DATA_PATH}corn_climate_risk_futures_daily_master.csv')\n",
    "temp_df['date_on'] = pd.to_datetime(temp_df['date_on'])\n",
    "\n",
    "# Add basic features (same as sample submission)\n",
    "temp_df['day_of_year'] = temp_df['date_on'].dt.dayofyear\n",
    "temp_df['quarter'] = temp_df['date_on'].dt.quarter\n",
    "\n",
    "# Merge market share\n",
    "temp_df = temp_df.merge(\n",
    "    market_share_df[['region_id', 'percent_country_production']], \n",
    "    on='region_id', how='left'\n",
    ")\n",
    "temp_df['percent_country_production'] = temp_df['percent_country_production'].fillna(1.0)\n",
    "\n",
    "# Create base risk scores (same as sample submission)\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    low_col = f'climate_risk_cnt_locations_{risk_type}_risk_low'\n",
    "    med_col = f'climate_risk_cnt_locations_{risk_type}_risk_medium' \n",
    "    high_col = f'climate_risk_cnt_locations_{risk_type}_risk_high'\n",
    "    \n",
    "    total = temp_df[low_col] + temp_df[med_col] + temp_df[high_col]\n",
    "    risk_score = (temp_df[med_col] + 2 * temp_df[high_col]) / (total + 1e-6)\n",
    "    weighted = risk_score * (temp_df['percent_country_production'] / 100)\n",
    "    \n",
    "    temp_df[f'climate_risk_{risk_type}_score'] = risk_score\n",
    "    temp_df[f'climate_risk_{risk_type}_weighted'] = weighted\n",
    "\n",
    "# Create composite indices\n",
    "score_cols = [f'climate_risk_{r}_score' for r in RISK_CATEGORIES]\n",
    "temp_df['climate_risk_temperature_stress'] = temp_df[['climate_risk_heat_stress_score', 'climate_risk_unseasonably_cold_score']].max(axis=1)\n",
    "temp_df['climate_risk_precipitation_stress'] = temp_df[['climate_risk_excess_precip_score', 'climate_risk_drought_score']].max(axis=1)\n",
    "temp_df['climate_risk_overall_stress'] = temp_df[score_cols].max(axis=1)\n",
    "temp_df['climate_risk_combined_stress'] = temp_df[score_cols].mean(axis=1)\n",
    "\n",
    "# Sort for rolling operations\n",
    "temp_df = temp_df.sort_values(['region_id', 'date_on'])\n",
    "\n",
    "# Create rolling features (7, 14, 30 days - same as sample submission)\n",
    "for window in [7, 14, 30]:\n",
    "    for risk_type in RISK_CATEGORIES:\n",
    "        score_col = f'climate_risk_{risk_type}_score'\n",
    "        temp_df[f'climate_risk_{risk_type}_ma_{window}d'] = (\n",
    "            temp_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        )\n",
    "        temp_df[f'climate_risk_{risk_type}_max_{window}d'] = (\n",
    "            temp_df.groupby('region_id')[score_col]\n",
    "            .transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        )\n",
    "\n",
    "# Create momentum features (same as sample submission)\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    temp_df[f'climate_risk_{risk_type}_change_1d'] = temp_df.groupby('region_id')[score_col].diff(1)\n",
    "    temp_df[f'climate_risk_{risk_type}_change_7d'] = temp_df.groupby('region_id')[score_col].diff(7)\n",
    "    temp_df[f'climate_risk_{risk_type}_acceleration'] = temp_df.groupby('region_id')[f'climate_risk_{risk_type}_change_1d'].diff(1)\n",
    "\n",
    "# Create country aggregations (same as sample submission)\n",
    "for risk_type in RISK_CATEGORIES:\n",
    "    score_col = f'climate_risk_{risk_type}_score'\n",
    "    weighted_col = f'climate_risk_{risk_type}_weighted'\n",
    "    \n",
    "    country_agg = temp_df.groupby(['country_name', 'date_on']).agg({\n",
    "        score_col: ['mean', 'max', 'std'],\n",
    "        weighted_col: 'sum',\n",
    "        'percent_country_production': 'sum'\n",
    "    }).round(4)\n",
    "    \n",
    "    country_agg.columns = [f'country_{risk_type}_{\"_\".join(col).strip()}' for col in country_agg.columns]\n",
    "    country_agg = country_agg.reset_index()\n",
    "    \n",
    "    temp_df = temp_df.merge(country_agg, on=['country_name', 'date_on'], how='left')\n",
    "\n",
    "# Now dropna to get valid IDs (this is what sample submission does)\n",
    "valid_ids = temp_df.dropna()['ID'].tolist()\n",
    "print(f\"üìä Valid IDs from sample submission approach: {len(valid_ids):,}\")\n",
    "\n",
    "# Clean up\n",
    "del temp_df\n",
    "\n",
    "# Step 2: Fill all engineered features in merged_df with 0\n",
    "print(\"üìä Filling engineered features with 0...\")\n",
    "\n",
    "for col in ALL_NEW_FEATURES:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# Also fill any remaining NaN in climate_risk columns\n",
    "climate_cols = [c for c in merged_df.columns if c.startswith('climate_risk_')]\n",
    "for col in climate_cols:\n",
    "    if merged_df[col].isna().any():\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# Step 3: Filter to valid IDs\n",
    "print(\"üìä Filtering to valid IDs...\")\n",
    "\n",
    "# First, drop rows with NaN in futures columns (non-trading days)\n",
    "futures_cols = [c for c in merged_df.columns if c.startswith('futures_')]\n",
    "baseline_df = merged_df.dropna(subset=futures_cols)\n",
    "\n",
    "# Then filter to only valid IDs\n",
    "baseline_df = baseline_df[baseline_df['ID'].isin(valid_ids)]\n",
    "\n",
    "print(f\"üìä After NaN handling: {len(baseline_df):,} rows\")\n",
    "print(f\"üìä Expected rows: {REQUIRED_ROWS:,}\")\n",
    "print(f\"üìä Match: {'‚úÖ' if len(baseline_df) == REQUIRED_ROWS else '‚ùå'}\")\n",
    "print(f\"üìä Total new features: {len(ALL_NEW_FEATURES)}\")\n",
    "\n",
    "# Final verification\n",
    "if len(baseline_df) != REQUIRED_ROWS:\n",
    "    diff = len(baseline_df) - REQUIRED_ROWS\n",
    "    print(f\"\\n‚ö†Ô∏è Row count difference: {diff:+d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82a8b2",
   "metadata": {
    "papermill": {
     "duration": 0.008662,
     "end_time": "2026-01-30T17:49:59.452332",
     "exception": false,
     "start_time": "2026-01-30T17:49:59.443670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üìä Phase 12: Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc22fa0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:49:59.472433Z",
     "iopub.status.busy": "2026-01-30T17:49:59.471422Z",
     "iopub.status.idle": "2026-01-30T17:53:21.055055Z",
     "shell.execute_reply": "2026-01-30T17:53:21.053870Z"
    },
    "papermill": {
     "duration": 201.596254,
     "end_time": "2026-01-30T17:53:21.057970",
     "exception": false,
     "start_time": "2026-01-30T17:49:59.461716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing feature contributions (this takes ~3 minutes)...\n",
      "   Climate features: 147\n",
      "   Futures features: 17\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature contributions\n",
    "print(\"üìä Analyzing feature contributions (this takes ~3 minutes)...\")\n",
    "\n",
    "climate_cols = [c for c in baseline_df.columns if c.startswith('climate_risk_')]\n",
    "futures_cols = [c for c in baseline_df.columns if c.startswith('futures_')]\n",
    "\n",
    "print(f\"   Climate features: {len(climate_cols)}\")\n",
    "print(f\"   Futures features: {len(futures_cols)}\")\n",
    "\n",
    "feature_analysis = analyze_feature_contributions(baseline_df, climate_cols, futures_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb37b52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:53:21.077291Z",
     "iopub.status.busy": "2026-01-30T17:53:21.076930Z",
     "iopub.status.idle": "2026-01-30T17:53:21.092549Z",
     "shell.execute_reply": "2026-01-30T17:53:21.091327Z"
    },
    "papermill": {
     "duration": 0.027464,
     "end_time": "2026-01-30T17:53:21.094815",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.067351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîù TOP 25 Features by Significant Correlation Count:\n",
      "================================================================================\n",
      "                              feature  sig_count  total  sig_pct  max_corr  avg_sig_corr\n",
      "      climate_risk_drought_cumsum_90d         63   2244 2.807487    0.7766        0.6000\n",
      "          climate_risk_drought_ma_90d         58   2244 2.584670    0.7766        0.6047\n",
      "          climate_risk_drought_ma_60d         54   2244 2.406417    0.7336        0.5992\n",
      "      climate_risk_drought_cumsum_60d         53   2244 2.361854    0.7336        0.6029\n",
      "    climate_risk_excess_precip_ma_90d         51   2244 2.272727    0.6761        0.5475\n",
      "climate_risk_excess_precip_cumsum_90d         50   2244 2.228164    0.6761        0.5539\n",
      "    climate_risk_excess_precip_ma_60d         48   2244 2.139037    0.6126        0.5434\n",
      "climate_risk_excess_precip_cumsum_60d         47   2244 2.094474    0.6126        0.5463\n",
      "         climate_risk_drought_ema_30d         42   2244 1.871658    0.7081        0.5893\n",
      "      climate_risk_drought_cumsum_30d         41   2244 1.827094    0.7243        0.5934\n",
      "         climate_risk_drought_ema_46d         40   2244 1.782531    0.7225        0.6136\n",
      "          climate_risk_drought_ma_30d         39   2244 1.737968    0.7243        0.5978\n",
      "         climate_risk_drought_ema_14d         34   2244 1.515152    0.6465        0.5506\n",
      "          climate_risk_drought_ma_14d         31   2244 1.381462    0.6470        0.5561\n",
      "    climate_risk_excess_precip_ma_30d         28   2244 1.247772    0.5863        0.5331\n",
      "climate_risk_excess_precip_cumsum_30d         28   2244 1.247772    0.5863        0.5331\n",
      "   climate_risk_excess_precip_ema_46d         26   2244 1.158645    0.6391        0.5564\n",
      "   climate_risk_excess_precip_ema_30d         23   2244 1.024955    0.6202        0.5435\n",
      "   climate_risk_excess_precip_max_60d         19   2244 0.846702    0.6056        0.5384\n",
      "   climate_risk_excess_precip_vol_46d         18   2244 0.802139    0.6595        0.5783\n",
      "         climate_risk_drought_max_14d         17   2244 0.757576    0.6998        0.5451\n",
      "   climate_risk_excess_precip_vol_30d         17   2244 0.757576    0.5730        0.5342\n",
      "   climate_risk_excess_precip_max_90d         15   2244 0.668449    0.5785        0.5312\n",
      "         climate_risk_drought_max_30d         13   2244 0.579323    0.6474        0.5514\n",
      "   climate_risk_excess_precip_max_30d         13   2244 0.579323    0.5878        0.5283\n"
     ]
    }
   ],
   "source": [
    "# Show top features\n",
    "print(\"\\nüîù TOP 25 Features by Significant Correlation Count:\")\n",
    "print(\"=\"*80)\n",
    "print(feature_analysis.head(25).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63b82f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:53:21.114527Z",
     "iopub.status.busy": "2026-01-30T17:53:21.114165Z",
     "iopub.status.idle": "2026-01-30T17:53:21.124856Z",
     "shell.execute_reply": "2026-01-30T17:53:21.123694Z"
    },
    "papermill": {
     "duration": 0.022814,
     "end_time": "2026-01-30T17:53:21.127124",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.104310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå BOTTOM 25 Features (candidates for removal):\n",
      "================================================================================\n",
      "                                    feature  sig_count  total  sig_pct  max_corr  avg_sig_corr\n",
      "               climate_risk_heat_stress_log          0   1394      0.0    0.3070           0.0\n",
      "               climate_risk_drought_squared          0   2244      0.0    0.4267           0.0\n",
      "                   climate_risk_drought_log          0   2244      0.0    0.4930           0.0\n",
      "          climate_risk_precipitation_stress          0   2244      0.0    0.3790           0.0\n",
      "            climate_risk_temperature_stress          0   2193      0.0    0.3132           0.0\n",
      "                climate_risk_overall_stress          0   2244      0.0    0.3173           0.0\n",
      "               climate_risk_combined_stress          0   2244      0.0    0.3350           0.0\n",
      "                     climate_risk_temp_diff          0   2193      0.0    0.3132           0.0\n",
      "           climate_risk_heat_stress_squared          0   1394      0.0    0.3149           0.0\n",
      "          climate_risk_precip_drought_ratio          0   2244      0.0    0.3911           0.0\n",
      "                    climate_risk_season_sin          0   2244      0.0    0.4784           0.0\n",
      "              climate_risk_drought_seasonal          0   2244      0.0    0.4726           0.0\n",
      "        climate_risk_excess_precip_seasonal          0   2244      0.0    0.4931           0.0\n",
      "         climate_risk_heat_stress_change_1d          0   1428      0.0    0.1216           0.0\n",
      "         climate_risk_heat_stress_change_7d          0   1428      0.0    0.1518           0.0\n",
      "      climate_risk_heat_stress_acceleration          0   1445      0.0    0.0883           0.0\n",
      "   climate_risk_unseasonably_cold_change_1d          0   1564      0.0    0.1106           0.0\n",
      "   climate_risk_unseasonably_cold_change_7d          0   1564      0.0    0.2036           0.0\n",
      "climate_risk_unseasonably_cold_acceleration          0   1598      0.0    0.1121           0.0\n",
      "       climate_risk_excess_precip_change_1d          0   2244      0.0    0.1095           0.0\n",
      "       climate_risk_excess_precip_change_7d          0   2244      0.0    0.2007           0.0\n",
      "    climate_risk_excess_precip_acceleration          0   2244      0.0    0.1025           0.0\n",
      "             climate_risk_drought_change_1d          0   2244      0.0    0.1381           0.0\n",
      "             climate_risk_drought_change_7d          0   2244      0.0    0.2462           0.0\n",
      "          climate_risk_drought_acceleration          0   2244      0.0    0.1473           0.0\n"
     ]
    }
   ],
   "source": [
    "# Show bottom features (candidates for removal)\n",
    "print(\"\\n‚ùå BOTTOM 25 Features (candidates for removal):\")\n",
    "print(\"=\"*80)\n",
    "print(feature_analysis.tail(25).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c57fadf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:53:21.145686Z",
     "iopub.status.busy": "2026-01-30T17:53:21.145299Z",
     "iopub.status.idle": "2026-01-30T17:53:21.154577Z",
     "shell.execute_reply": "2026-01-30T17:53:21.153455Z"
    },
    "papermill": {
     "duration": 0.021241,
     "end_time": "2026-01-30T17:53:21.156704",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.135463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Feature Selection Summary:\n",
      "   Total climate features: 147\n",
      "   Features with 0 significant correlations: 73\n",
      "   Features to remove: 61\n",
      "   Total significant correlations: 1052\n"
     ]
    }
   ],
   "source": [
    "# Identify features to remove\n",
    "zero_sig_features = feature_analysis[feature_analysis['sig_count'] == 0]['feature'].tolist()\n",
    "\n",
    "# Keep original cnt_locations columns (required by competition)\n",
    "original_cols = [c for c in zero_sig_features if 'cnt_locations' in c]\n",
    "FEATURES_TO_REMOVE = [c for c in zero_sig_features if c not in original_cols]\n",
    "\n",
    "print(f\"\\nüìä Feature Selection Summary:\")\n",
    "print(f\"   Total climate features: {len(climate_cols)}\")\n",
    "print(f\"   Features with 0 significant correlations: {len(zero_sig_features)}\")\n",
    "print(f\"   Features to remove: {len(FEATURES_TO_REMOVE)}\")\n",
    "print(f\"   Total significant correlations: {feature_analysis['sig_count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b50e",
   "metadata": {
    "papermill": {
     "duration": 0.009108,
     "end_time": "2026-01-30T17:53:21.174237",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.165129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üìä Phase 13: Create Optimized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2180de58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:53:21.194100Z",
     "iopub.status.busy": "2026-01-30T17:53:21.193680Z",
     "iopub.status.idle": "2026-01-30T17:53:21.611950Z",
     "shell.execute_reply": "2026-01-30T17:53:21.610744Z"
    },
    "papermill": {
     "duration": 0.43172,
     "end_time": "2026-01-30T17:53:21.615657",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.183937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Climate features: 147 ‚Üí 86 (removed 61)\n"
     ]
    }
   ],
   "source": [
    "# Create optimized dataset by removing weak features\n",
    "optimized_df = baseline_df.copy()\n",
    "\n",
    "cols_before = len([c for c in optimized_df.columns if c.startswith('climate_risk_')])\n",
    "optimized_df = optimized_df.drop(columns=FEATURES_TO_REMOVE, errors='ignore')\n",
    "cols_after = len([c for c in optimized_df.columns if c.startswith('climate_risk_')])\n",
    "\n",
    "print(f\"üìä Climate features: {cols_before} ‚Üí {cols_after} (removed {cols_before - cols_after})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acf001",
   "metadata": {
    "papermill": {
     "duration": 0.00993,
     "end_time": "2026-01-30T17:53:21.638729",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.628799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üìä Phase 14: Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f443ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:53:21.662943Z",
     "iopub.status.busy": "2026-01-30T17:53:21.662537Z",
     "iopub.status.idle": "2026-01-30T17:58:24.451389Z",
     "shell.execute_reply": "2026-01-30T17:58:24.450245Z"
    },
    "papermill": {
     "duration": 302.812937,
     "end_time": "2026-01-30T17:58:24.463236",
     "exception": false,
     "start_time": "2026-01-30T17:53:21.650299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Computing CFCS scores...\n",
      "\n",
      "Baseline (all features):\n",
      "CFCS: 51.58 | Sig: 1052/290564 (0.36%) | Features: 147\n",
      "\n",
      "Optimized (weak features removed):\n",
      "CFCS: 51.62 | Sig: 1052/177480 (0.59%) | Features: 86\n",
      "\n",
      "üìà IMPROVEMENT!\n",
      "   Delta: +0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Computing CFCS scores...\\n\")\n",
    "\n",
    "print(\"Baseline (all features):\")\n",
    "baseline_score = compute_cfcs(baseline_df)\n",
    "\n",
    "print(\"\\nOptimized (weak features removed):\")\n",
    "optimized_score = compute_cfcs(optimized_df)\n",
    "\n",
    "improvement = optimized_score['cfcs'] - baseline_score['cfcs']\n",
    "print(f\"\\n{'üìà IMPROVEMENT!' if improvement > 0 else 'üìâ No improvement'}\")\n",
    "print(f\"   Delta: {improvement:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3279dfe",
   "metadata": {
    "papermill": {
     "duration": 0.00831,
     "end_time": "2026-01-30T17:58:24.480168",
     "exception": false,
     "start_time": "2026-01-30T17:58:24.471858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## üìä Phase 15: Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e84c82e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:58:24.498660Z",
     "iopub.status.busy": "2026-01-30T17:58:24.498339Z",
     "iopub.status.idle": "2026-01-30T17:58:24.504923Z",
     "shell.execute_reply": "2026-01-30T17:58:24.503688Z"
    },
    "papermill": {
     "duration": 0.018423,
     "end_time": "2026-01-30T17:58:24.507053",
     "exception": false,
     "start_time": "2026-01-30T17:58:24.488630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best version: optimized (CFCS: 51.62)\n"
     ]
    }
   ],
   "source": [
    "# Select best version\n",
    "if optimized_score['cfcs'] >= baseline_score['cfcs']:\n",
    "    best_df = optimized_df\n",
    "    best_score = optimized_score\n",
    "    best_name = 'optimized'\n",
    "else:\n",
    "    best_df = baseline_df\n",
    "    best_score = baseline_score\n",
    "    best_name = 'baseline'\n",
    "\n",
    "print(f\"üèÜ Best version: {best_name} (CFCS: {best_score['cfcs']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b4f4d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:58:24.526468Z",
     "iopub.status.busy": "2026-01-30T17:58:24.525873Z",
     "iopub.status.idle": "2026-01-30T17:58:25.007391Z",
     "shell.execute_reply": "2026-01-30T17:58:25.006250Z"
    },
    "papermill": {
     "duration": 0.493463,
     "end_time": "2026-01-30T17:58:25.009366",
     "exception": false,
     "start_time": "2026-01-30T17:58:24.515903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ SUBMISSION VALIDATION\n",
      "============================================================\n",
      "‚úÖ Row count: 219,161/219,161\n",
      "‚úÖ ID column: True\n",
      "‚úÖ No nulls: 0 nulls\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "REQUIRED_ROWS = 219161\n",
    "submission = best_df.copy()\n",
    "\n",
    "# Safety: fill any remaining nulls\n",
    "if submission.isnull().sum().sum() > 0:\n",
    "    print(\"‚ö†Ô∏è Filling remaining nulls with 0...\")\n",
    "    submission = submission.fillna(0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SUBMISSION VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = [\n",
    "    ('Row count', len(submission) == REQUIRED_ROWS, f\"{len(submission):,}/{REQUIRED_ROWS:,}\"),\n",
    "    ('ID column', 'ID' in submission.columns, str('ID' in submission.columns)),\n",
    "    ('No nulls', submission.isnull().sum().sum() == 0, f\"{submission.isnull().sum().sum()} nulls\"),\n",
    "]\n",
    "\n",
    "for name, passed, detail in checks:\n",
    "    print(f\"{'‚úÖ' if passed else '‚ùå'} {name}: {detail}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1f6b688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:58:25.029161Z",
     "iopub.status.busy": "2026-01-30T17:58:25.028083Z",
     "iopub.status.idle": "2026-01-30T17:58:57.801078Z",
     "shell.execute_reply": "2026-01-30T17:58:57.799794Z"
    },
    "papermill": {
     "duration": 32.795085,
     "end_time": "2026-01-30T17:58:57.813166",
     "exception": false,
     "start_time": "2026-01-30T17:58:25.018081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Saved: /kaggle/working/submission.csv\n",
      "   Version: optimized\n",
      "   CFCS: 51.62\n",
      "   Rows: 219,161\n",
      "   Climate features: 86\n",
      "   Significant correlations: 1052/177480 (0.59%)\n"
     ]
    }
   ],
   "source": [
    "# Save submission\n",
    "output_file = f'{OUTPUT_PATH}submission.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "climate_features = [c for c in submission.columns if c.startswith('climate_risk_')]\n",
    "\n",
    "print(f\"\\nüìÅ Saved: {output_file}\")\n",
    "print(f\"   Version: {best_name}\")\n",
    "print(f\"   CFCS: {best_score['cfcs']}\")\n",
    "print(f\"   Rows: {len(submission):,}\")\n",
    "print(f\"   Climate features: {len(climate_features)}\")\n",
    "print(f\"   Significant correlations: {best_score['sig_count']}/{best_score['total']} ({best_score['sig_pct']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6667",
   "metadata": {
    "papermill": {
     "duration": 0.008622,
     "end_time": "2026-01-30T17:58:57.830316",
     "exception": false,
     "start_time": "2026-01-30T17:58:57.821694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üèÅ Conclusion: Synthesis & Strategic Outlook\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Performance Summary: Optimized Feature Architecture\n",
    "\n",
    "Our iterative engineering approach has yielded a refined feature set that directly addresses the nuances of the **CFCS** metric. By prioritizing **Signal Density** over sheer volume, we have successfully developed a methodology that maximizes correlation while minimizing noise.\n",
    "\n",
    "| Strategy Component | Impact on CFCS | Technical Validation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Temporal Alignment** | Increases `Max_Corr` | Lag & EMA features capture the price-discovery delay after climate shocks. |\n",
    "| **Non-linear Modeling** | Boosts `Avg_Sig_Corr` | Interaction terms (e.g., Heat √ó Drought) identify compounding stress events. |\n",
    "| **Strategic Pruning** | Optimizes `Sig_Count%` | Systematically eliminates zero-signal features to prevent denominator inflation. |\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Final Insights\n",
    "\n",
    "1.  **Drought & Excess Precip as Primary Drivers:** Our analysis indicates that hydrological extremes currently exhibit the strongest and most consistent predictive power for corn futures price movements.\n",
    "2.  **Quality-First Paradigm:** In the context of the CFCS metric, the \"more is better\" approach to features is counter-productive. A lean, high-conviction feature set is essential for achieving a top-tier leaderboard position.\n",
    "3.  **Seasonality is Key:** Encoding the sin/cos periodicity of the growing season has significantly stabilized our model's awareness of *when* a weather event becomes an economic catastrophe.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Future Horizons\n",
    "\n",
    "Moving forward, the integration of **Regional Production Weighting** and **Cross-Commodity Lead/Lag Analysis** (e.g., using Soybeans as a leading indicator for Corn sentiment) represents the next frontier for this solution.\n",
    "\n",
    "We believe that by continuing to bridge the gap between proprietary climate intelligence and market microstructure, we can unlock even higher levels of alpha in the agricultural futures space.\n",
    "\n",
    "**Good luck to all participants! üåΩüìà**\n",
    "\n",
    "---\n",
    "*Authored by Yehoshua*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3f677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:58:57.850541Z",
     "iopub.status.busy": "2026-01-30T17:58:57.850178Z",
     "iopub.status.idle": "2026-01-30T17:58:57.854818Z",
     "shell.execute_reply": "2026-01-30T17:58:57.853865Z"
    },
    "papermill": {
     "duration": 0.017736,
     "end_time": "2026-01-30T17:58:57.857090",
     "exception": false,
     "start_time": "2026-01-30T17:58:57.839354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15008526,
     "sourceId": 126158,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 567.388936,
   "end_time": "2026-01-30T17:58:58.590410",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-30T17:49:31.201474",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
