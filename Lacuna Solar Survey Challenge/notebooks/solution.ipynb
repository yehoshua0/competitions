{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"654c794449bd42378ad0ff1ba20a3842":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77ac0e5761554e789f001c1045129ea8","IPY_MODEL_3db63fd099ba4a2a8d3ffcaa147b6011","IPY_MODEL_5a5479ef19af4814b4a227ea44ffd90a"],"layout":"IPY_MODEL_60f4949511b44ae499946129cee86819"}},"77ac0e5761554e789f001c1045129ea8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69079a20dea64e91885053bf2cc54a4c","placeholder":"​","style":"IPY_MODEL_5ca7b048742c49e986dd8e30727367a7","value":"model.safetensors: 100%"}},"3db63fd099ba4a2a8d3ffcaa147b6011":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f26197afbe3469cb2d27ea32f7257ee","max":31471874,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d026f74db1264804b09f4b93da7bf720","value":31471874}},"5a5479ef19af4814b4a227ea44ffd90a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52204a5e84f419994c7017fc2b3bbf8","placeholder":"​","style":"IPY_MODEL_0b074c585dd5460f892711429f1d78aa","value":" 31.5M/31.5M [00:00&lt;00:00, 56.8MB/s]"}},"60f4949511b44ae499946129cee86819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69079a20dea64e91885053bf2cc54a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ca7b048742c49e986dd8e30727367a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f26197afbe3469cb2d27ea32f7257ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d026f74db1264804b09f4b93da7bf720":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e52204a5e84f419994c7017fc2b3bbf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b074c585dd5460f892711429f1d78aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10783793,"sourceType":"datasetVersion","datasetId":6691299}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"anJiNQwP6jq5","outputId":"7eed6c1a-fb25-47f9-afd6-f70a886dc94b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install -U albumentations -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport timm\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.optim as optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"Y5T_1zXS9HRb","outputId":"a8747c79-80d9-4263-fcd2-a410ab24388e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read in the training dataset\ntrain = pd.read_csv(f\"/kaggle/input/lacuna-solar-survey-challenge/Train.csv\")","metadata":{"id":"8eaj51iFv95-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"id":"saWCS23p2EEK","outputId":"c160cee4-db1c-4950-c8ce-4b1a3fcaaf27","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a placement mapper\nplacement_mapper = train[[\"ID\", \"placement\"]].drop_duplicates().set_index(\"ID\").to_dict()\n# Create a \"img_origin\" mapper\nimg_origin_mapper = train[[\"ID\", \"img_origin\"]].drop_duplicates().set_index(\"ID\").to_dict()\n\n# Group by \"ID\" and sum up boil_nb, pan_nbr\ntrain_df = train.groupby(\"ID\").sum().reset_index()[[\"ID\", \"boil_nbr\", \"pan_nbr\"]]\n\n# Map img_origin and placement\ntrain_df[\"img_origin\"] = train_df[\"ID\"].map(img_origin_mapper[\"img_origin\"])\ntrain_df[\"placement\"] = train_df[\"ID\"].map(placement_mapper[\"placement\"])\n\n# Create path column\ntrain_df[\"path\"] = \"/kaggle/input/lacuna-solar-survey-challenge/images/\" + train_df[\"ID\"] + \".jpg\"","metadata":{"id":"5DF3hPzM2wIG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"ULcbkzcQ-T86","outputId":"59ca52bc-f36e-41ec-9590-a913ea89fd88","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Prepare metadata encoders\ndef prepare_metadata_encoders(train_df):\n    # Encode img_origin (D/S)\n    img_origin_encoder = LabelEncoder()\n    img_origin_encoder.fit(train_df['img_origin'])\n    \n    # Encode placement (roof/openspace/r_openspace/ground)\n    placement_encoder = LabelEncoder()\n    placement_encoder.fit(train_df['placement'])\n    \n    return img_origin_encoder, placement_encoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Setup with Modified Model\nimg_origin_encoder, placement_encoder = prepare_metadata_encoders(train_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stratified KFold based on multi-label targets\ntrain_df[\"stratify_label\"] = train_df[[\"boil_nbr\", \"pan_nbr\"]].sum(axis=1)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntrain_df[\"fold\"] = -1\nfor fold, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"stratify_label\"])):\n    train_df.loc[valid_idx, \"fold\"] = fold","metadata":{"id":"HKG3kLmc3U0j","outputId":"dec2c167-1473-4413-c7c6-68fc5b9ef7a8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced Augmentation Pipeline\ntrain_transforms = A.Compose([\n    A.Resize(384, 384),\n    # Dynamic Spatial Transforms (Geometric)\n    A.Rotate(limit=30, p=0.5),              # Rotate ±30 degrees\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.3),\n    # Color Transforms\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n    # Targeted Dropout (Cutout)\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),  # Randomly drop small regions\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Resize(384, 384),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modified Dataset to include metadata\nclass SolarPanelDataset(Dataset):\n    def __init__(self, dataframe, transform=None, to_train=True, \n                 img_origin_encoder=None, placement_encoder=None):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.to_train = to_train\n        self.img_origin_encoder = img_origin_encoder\n        self.placement_encoder = placement_encoder\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image = cv2.imread(row[\"path\"])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n\n        # Metadata\n        img_origin = self.img_origin_encoder.transform([row[\"img_origin\"]])[0]\n        placement = self.placement_encoder.transform([row[\"placement\"]])[0]\n        metadata = torch.tensor([img_origin, placement], dtype=torch.long)\n\n        if self.to_train:\n            target = torch.tensor([row[\"boil_nbr\"], row[\"pan_nbr\"]], dtype=torch.float32)\n            return image, metadata, target\n        else:\n            return image, metadata","metadata":{"id":"HihBcjfh9RH7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare Dataloaders\nfold = 0  # Change fold index as needed\ntrain_data = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\nvalid_data = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n\n# Update dataloaders with metadata\ndataset_train = SolarPanelDataset(\n    train_data, \n    transform=train_transforms,\n    img_origin_encoder=img_origin_encoder,\n    placement_encoder=placement_encoder\n)\ndataset_valid = SolarPanelDataset(\n    valid_data, \n    transform=test_transforms,\n    img_origin_encoder=img_origin_encoder,\n    placement_encoder=placement_encoder\n)\n\n# ensure num_worker = os.cpu_count() // 2\ntrain_loader = DataLoader(dataset_train, batch_size=32, shuffle=True, num_workers=os.cpu_count())\nvalid_loader = DataLoader(dataset_valid, batch_size=32, shuffle=False)","metadata":{"id":"5zg6m_dc9Va7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model with Metadata Integration and Fusion\nclass EfficientNetV2Regressor(nn.Module):\n    def __init__(self, num_img_origins=2, num_placements=4):\n        super(EfficientNetV2Regressor, self).__init__()\n        \n        # 2. Image Feature Extraction with EfficientNetV2\n        self.backbone = timm.create_model('tf_efficientnetv2_s', pretrained=True)\n        # Get the feature dimension from the backbone\n        with torch.no_grad():\n            # This is a dummy forward pass to get the feature size\n            dummy_input = torch.zeros(1, 3, 384, 384)\n            features = self.backbone.forward_features(dummy_input)\n            self.feature_dim = features.shape[1] * features.shape[2] * features.shape[3]\n        \n        # Modify the classifier to output features\n        self.backbone.global_pool = nn.Identity()\n        self.backbone.classifier = nn.Identity()\n\n        # 1. Metadata Integration\n        # Embedding layers for categorical variables\n        self.img_origin_embed = nn.Embedding(num_img_origins, 8)  # 8-dimensional embedding\n        self.placement_embed = nn.Embedding(num_placements, 16)   # 16-dimensional embedding\n        \n        # Total metadata embedding size\n        metadata_dim = 8 + 16  # 24\n        \n        # 3. Fusion - 2-layer regression head\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(self.feature_dim + metadata_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)  # Output 2 values: boil_nbr, pan_nbr\n        )\n\n    def forward(self, x, metadata):\n        # Image feature extraction\n        features = self.backbone(x)  # Shape: [batch_size, channels, height, width]\n        features = features.view(features.size(0), -1)  # Flatten: [batch_size, feature_dim]\n\n        # Metadata embeddings\n        img_origin_emb = self.img_origin_embed(metadata[:, 0])  # [batch_size, 8]\n        placement_emb = self.placement_embed(metadata[:, 1])    # [batch_size, 16]\n        \n        # Concatenate metadata embeddings\n        metadata_features = torch.cat([img_origin_emb, placement_emb], dim=1)  # [batch_size, 24]\n\n        # Fusion\n        combined = torch.cat([features, metadata_features], dim=1)  # [batch_size, feature_dim + 24]\n        output = self.fusion_layer(combined)  # [batch_size, 2]\n        \n        return output","metadata":{"id":"urT-xV7X_tKB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nmodel = EfficientNetV2Regressor(\n    num_img_origins=len(img_origin_encoder.classes_),\n    num_placements=len(placement_encoder.classes_)\n).cuda()\n\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nbest_model_path = \"best_model_v2.pth\"","metadata":{"id":"DqOjmljZ_wqQ","outputId":"e79d1147-5c89-4b26-be64-c4e87272d0eb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Data Strategy: Full Cross-Validation Loop\ndef train_cross_validation(train_df, n_splits=5, num_epochs=50):\n    # Prepare metadata encoders\n    img_origin_encoder = LabelEncoder()\n    placement_encoder = LabelEncoder()\n    img_origin_encoder.fit(train_df['img_origin'])\n    placement_encoder.fit(train_df['placement'])\n\n    # Stratified K-Fold\n    train_df[\"stratify_label\"] = train_df[[\"boil_nbr\", \"pan_nbr\"]].sum(axis=1)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    fold_mae_scores = []\n    \n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"stratify_label\"])):\n        print(f\"\\nTraining Fold {fold + 1}/{n_splits}\")\n        \n        # Split data\n        train_data = train_df.iloc[train_idx].reset_index(drop=True)\n        valid_data = train_df.iloc[valid_idx].reset_index(drop=True)\n\n        # Create datasets\n        dataset_train = SolarPanelDataset(\n            train_data, transform=train_transforms,\n            img_origin_encoder=img_origin_encoder,\n            placement_encoder=placement_encoder\n        )\n        dataset_valid = SolarPanelDataset(\n            valid_data, transform=test_transforms,\n            img_origin_encoder=img_origin_encoder,\n            placement_encoder=placement_encoder\n        )\n\n        # DataLoaders\n        train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n        valid_loader = DataLoader(dataset_valid, batch_size=32, shuffle=False)\n\n        # Initialize model\n        model = EfficientNetV2Regressor(\n            num_img_origins=len(img_origin_encoder.classes_),\n            num_placements=len(placement_encoder.classes_)\n        ).cuda()\n        \n        criterion = nn.L1Loss()\n        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n        best_model_path = f\"best_model_fold_{fold}.pth\"\n        best_loss = float(\"inf\")\n\n        # Training Loop\n        for epoch in range(num_epochs):\n            model.train()\n            epoch_loss = 0.0\n            for images, metadata, targets in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs} - Training\"):\n                images, metadata, targets = images.cuda(), metadata.cuda(), targets.cuda()\n                optimizer.zero_grad()\n                outputs = model(images, metadata)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            # Validation Loop\n            model.eval()\n            val_loss = 0.0\n            preds, true_vals = [], []\n            with torch.no_grad():\n                for images, metadata, targets in tqdm(valid_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs} - Validation\"):\n                    images, metadata, targets = images.cuda(), metadata.cuda(), targets.cuda()\n                    outputs = model(images, metadata)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item()\n                    preds.append(outputs.cpu().numpy())\n                    true_vals.append(targets.cpu().numpy())\n\n            val_loss /= len(valid_loader)\n            preds = np.concatenate(preds, axis=0)\n            true_vals = np.concatenate(true_vals, axis=0)\n            mae = mean_absolute_error(true_vals, preds)\n            \n            print(f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, MAE: {mae:.4f}\")\n\n            if val_loss < best_loss:\n                best_loss = val_loss\n                torch.save(model.state_dict(), best_model_path)\n\n        fold_mae_scores.append(mae)\n    \n    avg_mae = np.mean(fold_mae_scores)\n    print(f\"\\nCross-Validation Complete! Average MAE across {n_splits} folds: {avg_mae:.4f}\")\n    return fold_mae_scores, avg_mae\n\n# Execute Cross-Validation\nfold_mae_scores, avg_mae = train_cross_validation(train_df, n_splits=5, num_epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test Set Prediction\ntest_df = pd.read_csv(\"/kaggle/input/lacuna-solar-survey-challenge/Test.csv\")\ntest_df[\"path\"] = \"/kaggle/input/lacuna-solar-survey-challenge/images/\" + test_df[\"ID\"] + \".jpg\"\n\ndataset_test = SolarPanelDataset(\n    test_df, transform=test_transforms, to_train=False,\n    img_origin_encoder=img_origin_encoder,\n    placement_encoder=placement_encoder\n)\ntest_loader = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=os.cpu_count() // 2)\n\n# Step 1: Load all fold models\nnum_folds = 5  # Adjust based on your k-fold setup (e.g., 5-fold CV)\nmodels = []\nfor fold in range(num_folds):\n    model = EfficientNetV2Regressor(\n        num_img_origins=len(img_origin_encoder.classes_),\n        num_placements=len(placement_encoder.classes_)\n    ).cuda()\n    best_model_path = f\"best_model_fold_{fold}.pth\"\n    model.load_state_dict(torch.load(best_model_path))\n    model.eval()  # Set to evaluation mode\n    models.append(model)\n\n# Step 2: Ensemble predictions across all fold models\ntest_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Predicting on Test Set\"):\n        images, metadata = batch  # Unpack images and metadata\n        images = images.cuda()\n        metadata = metadata.cuda()  # Move metadata to GPU\n\n        # Get predictions from each model and average them\n        batch_preds = []\n        for model in models:\n            outputs = model(images, metadata).cpu().numpy()  # Pass both inputs to model\n            batch_preds.append(outputs)\n        \n        # Average predictions across all models for this batch\n        batch_preds = np.stack(batch_preds, axis=0)  # Shape: (num_models, batch_size, output_dim)\n        ensemble_batch_pred = np.mean(batch_preds, axis=0)  # Shape: (batch_size, output_dim)\n        test_preds.append(ensemble_batch_pred)\n\n# Step 3: Concatenate predictions across all batches\ntest_preds = np.concatenate(test_preds, axis=0)\nprint(f\"Test predictions shape: {test_preds.shape}\")","metadata":{"id":"ne1bkGIvKuAG","outputId":"67438760-c2bd-4dfc-edb9-d0e0611ce7e3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Submission\nsubmission = pd.DataFrame()\nsubmission[\"ID\"] = np.repeat(test_df[\"ID\"].values, 2)\nsubmission[\"ID\"] = submission[\"ID\"] + np.tile([\"_boil\", \"_pan\"], len(test_df))\nsubmission[\"Target\"] = test_preds.flatten().clip(0,1000)\n\n# Save Submission\nsubmission.to_csv(\"Submission.csv\", index=False)\nprint(\"Submission file saved!\")","metadata":{"id":"cepD5bBiP2Fz","outputId":"e833f539-2ec0-447f-e68d-1aea953c5e93","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}